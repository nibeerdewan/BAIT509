Answer the following questions and upload your results to your github repo. Remember, your answers do not have to be correct to earn participation points!
•	Bagging is a special case of random forests under which case?
Random forests de-correlates the trees. Random forests choose m=sqrt(p) predictors out of the possible p predictors. The main difference between bagging and random forests is the choice of predictor size m. When m=p, this is the same as bagging. 
•	What are the hyperparameters we can control for random forests?
We can control the depth of the tree (d), number of random predictors (m), number of trees (n) and the randomizer seed (r).
•	Suppose you have the following paired data of (x,y): (1,2), (1,5), (2,0). Which of the following are valid bootstrapped data sets? Why/why not?
i.	(1,0), (1,2), (1,5) – not valid: the first observation is not contained in the original dataset. 
ii.	(1,2), (2,0) – not valid: Only has 2 observations. 
iii.	(1,2), (1,2), (1,5) – valid 
•	For each of the above valid bootstrapped data sets, which observations are out-of-bag (OOB)?
(2,0) is OOB. 
•	You make a random forest consisting of four trees. You obtain a new observation of predictors and would like to predict the response. What would your prediction be in the following cases?
i.	Regression: your trees make the following four predictions: 1,1,3,3.
	2 is the overall prediction because it is an average. 
ii.	Classification: your trees make the following four predictions: "A", "A", "B", "C".
	A is the prediction because more number of occurrences. 

